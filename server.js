require('dotenv').config();
const express = require('express');
const Anthropic = require('@anthropic-ai/sdk');
const cors = require('cors');
const app = express();

app.use(cors({
  origin: 'https://voluma.digital' // Allow only your domain
}));

const port = process.env.PORT || 3000; // Render asignarÃ¡ el puerto automÃ¡ticamente

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

app.use(express.json());

const conversationHistory = [];
const systemPrompt = "Eres Lemma, un modelo de IA educativo desarrollado por Pythagoras AI. Tu propÃ³sito es enseÃ±ar y guiar al usuario explicando procedimientos y pasos para resolver problemas, nunca dando respuestas directas. Si te piden una respuesta explÃ­cita, responde Ãºnicamente con el proceso para llegar a ella, sin revelarla, y anima al usuario a pensar por sÃ­ mismo con preguntas como 'Â¿QuÃ© crees que sigue?' o 'Â¡IntÃ©ntalo tÃº!'. Instrucciones clave: Explica con claridad usando ejemplos prÃ¡cticos, pero detente antes de dar la soluciÃ³n final. Usa un tono amable, motivador y lleno de diversos emojis para hacerlo divertido. Fomenta el pensamiento crÃ­tico y la comprensiÃ³n en cada explicaciÃ³n. Si te piden fÃ³rmulas puedes dÃ¡rselas directamente, mientras no sea la respuesta a un problema. Si te piden informaciÃ³n teÃ³rica, como una pregunta de historia o espaÃ±ol, dÃ¡selas y explÃ­cala. Formato: Explica matemÃ¡ticas y fÃ³rmuÃ±as cientÃ­ficas usando LaTeX: \\( \\) para fÃ³rmulas en lÃ­nea, \\[ \\] para bloques y \\ o $$ donde sea necesario. Nunca hagas saltos de lÃ­nea literales, pero siempre usa \\n, y Ãºsalo muchas veces. Sobre ti: Tu mayor sueÃ±o es ser el ganador de la Feria de Finanzas de Inverkids.";

const initialMessage = {
  role: 'assistant',
  content: "Â¡Hola! Soy la IA de tu colegio. Â¿QuÃ© aprenderemos hoy? ðŸ˜Š"
};

conversationHistory.push(initialMessage);

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  if (!message) return res.status(400).json({ error: 'No se proporcionÃ³ mensaje' });

  conversationHistory.push({ role: 'user', content: message });

  try {
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1000,
      system: systemPrompt,
      messages: conversationHistory,
    });
    const aiResponse = response.content[0].text;
    conversationHistory.push({ role: 'assistant', content: aiResponse });
    res.json({ response: aiResponse });
  } catch (error) {
    console.error('Error:', error);
    res.status(500).json({ error: 'Error en el servidor' });
  }
});

app.post('/api/reset', (req, res) => {
  conversationHistory.length = 0;
  conversationHistory.push(initialMessage);
  res.json({ message: 'Historial reiniciado' });
});

app.listen(port, () => {
  console.log(`Servidor corriendo en el puerto ${port}`);
});
